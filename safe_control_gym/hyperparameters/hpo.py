''' The implementation of HPO class

Reference:
    * stable baselines3 https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/rl_zoo3/hyperparams_opt.py
    * Optuna: https://optuna.org
'''
import os
from copy import deepcopy
from functools import partial

import matplotlib.pyplot as plt
import numpy as np
import optuna
import yaml
from optuna.samplers import RandomSampler, TPESampler
from optuna.study import MaxTrialsCallback
from optuna.trial import FrozenTrial, TrialState
from optuna.visualization.matplotlib import plot_optimization_history, plot_param_importances
from optuna_dashboard import run_server

from safe_control_gym.experiments.base_experiment import BaseExperiment
from safe_control_gym.hyperparameters.hpo_sampler import HYPERPARAMS_SAMPLER
from safe_control_gym.utils.logging import ExperimentLogger
from safe_control_gym.utils.registration import make


class HPO(object):

    def __init__(self, algo, task, sampler, load_study, output_dir, task_config, hpo_config, **algo_config):
        ''' Hyperparameter optimization class

        Args:
            algo: algo name
            env_func: environment that the agent will interact with
            output_dir: output directory
            hpo_config: hyperparameter optimization configuration
            algo_config: algorithm configuration
        '''

        self.algo = algo
        self.study_name = algo + '_hpo'
        self.task = task
        self.load_study = load_study
        self.task_config = task_config
        self.hpo_config = hpo_config
        self.hps_config = hpo_config.hps_config
        self.output_dir = output_dir
        self.algo_config = algo_config
        self.logger = ExperimentLogger(output_dir, log_file_out=False)
        self.total_runs = 0
        # init sampler
        if sampler == 'RandomSampler':
            self.sampler = RandomSampler(seed=self.hpo_config.seed)
        elif sampler == 'TPESampler':
            self.sampler = TPESampler(seed=self.hpo_config.seed)
        else:
            raise ValueError('Unknown sampler.')

        assert len(hpo_config.objective) == len(hpo_config.direction), 'objective and direction must have the same length'

    def objective(self, trial: optuna.Trial) -> float:
        ''' The stochastic objective function for a HPO tool to optimize over

        Args:
            trial: A single trial object that contains the hyperparameters to be evaluated
        '''

        # sample candidate hyperparameters
        sampled_hyperparams = HYPERPARAMS_SAMPLER[self.algo](self.hps_config, trial)

        # log trial number
        self.logger.info(f'Trial number: {trial.number}')

        # flag for increasing runs
        increase_runs = True
        first_iteration = True

        # do repetition
        returns, seeds = [], []
        while increase_runs:
            increase_runs = False
            if first_iteration:
                Gs = np.inf
            for i in range(self.hpo_config.repetitions):

                seed = np.random.randint(0, 10000)
                # update the agent config with sample candidate hyperparameters
                # new agent with the new hps
                for hp in sampled_hyperparams:
                    self.algo_config[hp] = sampled_hyperparams[hp]

                seeds.append(seed)
                self.logger.info(f'Sample hyperparameters: {sampled_hyperparams}')
                self.logger.info(f'Seeds: {seeds}')

                try:
                    self.env_func = partial(make, self.task, output_dir=self.output_dir, **self.task_config)
                    # using deepcopy(self.algo_config) prevent setting being overwritten
                    self.agent = make(self.algo,
                                      self.env_func,
                                      training=True,
                                      checkpoint_path=os.path.join(self.output_dir, 'model_latest.pt'),
                                      output_dir=os.path.join(self.output_dir, 'hpo'),
                                      use_gpu=self.hpo_config.use_gpu,
                                      seed=seed,
                                      **deepcopy(self.algo_config))

                    self.agent.reset()
                    eval_env = self.env_func(seed=seed * 111)
                    experiment = BaseExperiment(eval_env, self.agent)
                except Exception as e:
                    # catch exception
                    self.logger.info(f'Exception occurs when constructing agent: {e}')

                # return objective estimate
                # TODO: report intermediate results to Optuna for pruning
                try:
                    # self.agent.learn()
                    experiment.launch_training()
                    self.total_runs += 1

                except Exception as e:
                    # catch the NaN generated by the sampler
                    self.agent.close()
                    del self.agent
                    del self.env_func
                    del experiment
                    self.logger.info(f'Exception occurs during learning: {e}')
                    print(e)
                    print('Sampled hyperparameters:')
                    print(sampled_hyperparams)
                    returns.append(0.0)
                    break

                # avg_return = self.agent._run()
                _, metrics = experiment.run_evaluation(n_episodes=5, n_steps=None, done_on_max_steps=True)

                # at the moment, only single-objective optimization is supported
                returns.append(metrics[self.hpo_config.objective[0]])
                self.logger.info(f'Sampled objectives: {returns}')

                self.agent.close()
                # delete instances
                del self.agent
                del self.env_func

            Gss = self._compute_cvar(np.array(returns), self.hpo_config.alpha)

            # if the current objective is better than the best objective, trigger more runs to avoid maximization bias
            if self.hpo_config.warm_trials < len(self.study.trials) and self.hpo_config.dynamical_runs:
                if Gss > self.study.best_value or first_iteration is False:
                    if abs(Gs - Gss) > self.hpo_config.approximation_threshold:
                        increase_runs = True
                        first_iteration = False
                        Gs = Gss
                        self.logger.info('Trigger more runs')
                    else:
                        increase_runs = False

        self.logger.info(f'Returns: {Gss}')

        return Gss

    def hyperparameter_optimization(self) -> None:
        if self.load_study:
            self.study = optuna.load_study(study_name=self.study_name, storage=f'mysql+pymysql://optuna@localhost/{self.study_name}')
        elif self.hpo_config.use_database is False:
            # single-objective optimization
            if len(self.hpo_config.direction) == 1:
                self.study = optuna.create_study(
                    direction=self.hpo_config.direction[0],
                    sampler=self.sampler,
                    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),
                    study_name=self.study_name,
                )
            # multi-objective optimization
            else:
                self.study = optuna.create_study(
                    directions=self.hpo_config.direction,
                    sampler=self.sampler,
                    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),
                    study_name=self.study_name,
                )
        else:
            # single-objective optimization
            if len(self.hpo_config.direction) == 1:
                self.study = optuna.create_study(
                    direction=self.hpo_config.direction[0],
                    sampler=self.sampler,
                    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),
                    study_name=self.study_name,
                    storage=f'mysql+pymysql://optuna@localhost/{self.study_name}',
                    load_if_exists=self.hpo_config.load_if_exists
                )
            # multi-objective optimization
            else:
                self.study = optuna.create_study(
                    directions=self.hpo_config.direction,
                    sampler=self.sampler,
                    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),
                    study_name=self.study_name,
                    storage=f'mysql+pymysql://optuna@localhost/{self.study_name}',
                    load_if_exists=self.hpo_config.load_if_exists
                )

        self.study.optimize(self.objective,
                            catch=(RuntimeError,),
                            callbacks=[MaxTrialsCallback(self.hpo_config.trials, states=(TrialState.COMPLETE,))],
                            )

        output_dir = os.path.join(self.output_dir, 'hpo')
        # save meta data
        self.study.trials_dataframe().to_csv(output_dir + '/trials.csv')

        # save top-n best hyperparameters
        if len(self.hpo_config.direction) == 1:
            trials = self.study.trials
            if self.hpo_config.direction[0] == 'minimize':
                trials.sort(key=self._value_key)
            else:
                trials.sort(key=self._value_key, reverse=True)
            for i in range(min(self.hpo_config.save_n_best_hps, len(self.study.trials))):
                params = trials[i].params
                with open(f'{output_dir}/hyperparameters_{trials[i].value:.4f}.yaml', 'w')as f:
                    yaml.dump(params, f, default_flow_style=False)
        else:
            best_trials = self.study.best_trials
            for i in range(len(self.study.best_trials)):
                params = best_trials[i].params
                with open(f'{output_dir}/best_hyperparameters_[{best_trials[i].values[0]:.4f},{best_trials[i].values[1]:.4f}].yaml', 'w')as f:
                    yaml.dump(params, f, default_flow_style=False)

        # dashboard
        if self.hpo_config.dashboard and self.hpo_config.use_database:
            run_server(f'mysql+pymysql://optuna@localhost/{self.study_name}')

        # save plot
        try:
            if len(self.hpo_config.objective) == 1:
                plot_param_importances(self.study)
                plt.tight_layout()
                plt.savefig(output_dir + '/param_importances.png')
                # plt.show()
                plt.close()
                plot_optimization_history(self.study)
                plt.tight_layout()
                plt.savefig(output_dir + '/optimization_history.png')
                # plt.show()
                plt.close()
            else:
                for i in range(len(self.hpo_config.objective)):
                    plot_param_importances(self.study, target=lambda t: t.values[i])
                    plt.tight_layout()
                    plt.savefig(output_dir + f'/param_importances_{self.hpo_config.objective[i]}.png')
                    # plt.show()
                    plt.close()
                    plot_optimization_history(self.study, target=lambda t: t.values[i])
                    plt.tight_layout()
                    plt.savefig(output_dir + f'/optimization_history_{self.hpo_config.objective[i]}.png')
                    # plt.show()
                    plt.close()
        except Exception as e:
            print(e)
            print('Plotting failed.')

        self.logger.info(f'Total runs: {self.total_runs}')
        self.logger.close()

        return

    def _value_key(self, trial: FrozenTrial) -> float:
        ''' Returns value of trial object for sorting.'''
        if trial.value is None:
            if self.hpo_config.direction[0] == 'minimize':
                return float('inf')
            else:
                return float('-inf')
        else:
            return trial.value

    def _compute_cvar(self, returns: np.ndarray, alpha: float = 0.2) -> float:
        ''' Compute CVaR.'''
        assert returns.ndim == 1, 'returns must be 1D array'
        sorted_returns = np.sort(returns)
        n = len(sorted_returns)
        VaR_idx = int(alpha * n)
        if VaR_idx == 0:
            VaR_idx = 1

        if self.hpo_config.direction[0] == 'minimize':
            CVaR = sorted_returns[-VaR_idx:].mean()
        else:
            CVaR = sorted_returns[:VaR_idx].mean()

        return CVaR
