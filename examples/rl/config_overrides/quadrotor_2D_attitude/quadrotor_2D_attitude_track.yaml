task_config:
  seed: 1337
  info_in_reset: True
  ctrl_freq: 60
  pyb_freq: 1200
  physics: pyb
  quad_type: 4
  normalized_rl_action_space: False

  init_state:
    init_x: 0
    init_x_dot: 0
    init_z: 1.15
    init_z_dot: 0
    init_theta: 0
    init_theta_dot: 0
  randomized_init: True
  randomized_inertial_prop: False

  init_state_randomization_info:
    init_x:
      distrib: 'uniform'
      low: -0.01
      high: 0.01
    init_x_dot:
      distrib: 'uniform'
      low: -0.01
      high: 0.01
    init_z:
      distrib: 'uniform'
      low: 1.15
      high: 1.15
    init_z_dot:
      distrib: 'uniform'
      low: -0.01
      high: 0.01
    init_theta:
      distrib: 'uniform'
      low: -0.02
      high: 0.02
    init_theta_dot:
      distrib: 'uniform'
      low: -0.02
      high: 0.02

  task: traj_tracking
  task_info:
    trajectory_type: figure8
    num_cycles: 1
    trajectory_plane: 'xz'
    trajectory_position_offset: [0, 1.2]
    trajectory_scale: 0.5

  inertial_prop:
    M: 0.027
    Iyy: 1.4e-05

  episode_len_sec: 10
  cost: rl_reward
  obs_goal_horizon: 1

  # RL Reward
  rew_state_weight: [1.0, 0.01, 1.0, 0.01, 0.01, 0.01]
  rew_act_weight: 1.0
  rew_exponential: True

  constraints:
    - constraint_form: default_constraint
      constrained_variable: state
      upper_bounds: [2, 1, 2, 1, 0.2, 1.5]
      lower_bounds: [-2, -1, 0, -1, -0.2, -1.5]
    - constraint_form: default_constraint
      constrained_variable: input
      upper_bounds: [0.58, 0.8]
      lower_bounds: [0.06, -0.8]
  done_on_out_of_bound: False
  done_on_violation: False
